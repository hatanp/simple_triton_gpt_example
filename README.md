Simple example for running text generation with custom Python backend with NVIDIA Triton inference server

#### Setup

- Run setup.sh at server folder to save model files locally (TODO: make use of proper Hugging Face cache)
